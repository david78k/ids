Description

*************************************************************************

1. The steps we took to develop the code 

*************************************************************************
- 1) First started developing non-MR (MapReduce) pure Java implementation of the PageRank algorithm on local machine without Hadoop: thought that it was going to be easy to convert non-MR to MR.
- 2) MR with Hadoop on local machine: implemented the algorhtm with MapReduce functions, set up Hadoop on local machine and run the algorithm on a single node
- 3) EMR and S3 on AWS: uploaded the data and ran the alogirhtm on EMR
- we also worked from smaller data size to the original huge data that was given to us. For the small data size, we had to split the original data into small sizes (first 100 lines, 1000 lines, 10000lines, 1million lines, and 5 million lines). If we succeeded with the small data, we incrementally increased the data size.
 

*************************************************************************

2. Difficulties we faced and how we solved the problems 

*************************************************************************
- parsing xml page was not easy. at first, we were trying to use a parser library, but we couldn't get it to work. In the end, we parsed manually to get what we wanted. Then, we realized the need of the XMLInputFormat library provided by  the Mahout community which we finally used.

- too many specifications to follow: although simplified, there were still lots of instructions to obey. For example, file input/output name, which links to parse and not

- data manipulation: sort, order, key-valye mapping, indexing 
  
- creating a jar file: combining separate files (external jar files) into a single file was very complicated

- arranging, managing, and adjusting output directory layout was clumsy

- apache hadoop mapreduce related classes had dependency complex. setup/import of related classes was hard and time-consuming

- mapreduce class type input/output and key/value class type are very complex and confusing

- Although the required mapreduce API (version 1.0.3), which is very old, supports both old API (mapred.*) and new API (mapreduce.*), the old API doesn't work for some cases. should have been noted beforehand

- long procedure to run pagerank using EMR and S3: had to install many things to use command line tools (s3cmd and elastic-mapreduce) for running client scripts

- testing and debugging was a nightmare. every time the code is changed, it has to
  be compiled/jared/uploaded. the worst thing is to wait for the cluster to be initialized and run the jar application

- using s3 Java API to store output into s3bucket/results folder was very difficult

- output file arrangement becomes messed up. naming files to meet the requirements like outlinks.out, n.out ...

- passing multiple arguments using emr command line tool was not easy

- Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
file:/root/datascience/david78k-ids/results already exists
  It is frustrating that we can't overwrite existing directory. I had to create new directories for this. To solve the output file name conflict problems, I created files with the current date and time.  

- Writable data structure for a page that includes title and outlinks has complex format

- following the file formatting was the hardest part in this project, not the algorithms

- it was impossible to have two output classes during map phase in pagerank job. Hence, I made use of the Page data structure to contain all the outputs.

- the data given was so large, I coudn't handle the disk space even on local cluster machines. Thus, I reduced the hdfs replication factor to 1.

- As hadoop doesn't allow multiple input/output, i used string for both page ranks and links to store/recover the link graph from map phase to reduce phase. The problem with this approach is the precision of double number (decimal point below 17 digits). However, double number computation is not accurate anyways even with Double class.  

- shared variables for Mapper and Reducer using global variables in distributed mode does not work while they work in single node. so, I had to use Jobconf to set/get the variables for Mapper and Reducer class.

*************************************************************************

4. What we learned

*************************************************************************

- long process from input data to output data. there are too many steps, too many small things to take care of although the core algorithm is simple
- running the same code on a single local machine and distributed systems are somewhat different (e.g., different APIs for local file system and s3). static variable N is not set properly on EMR while it is set properly on local environment

- processing the small data and large data is also different (e.g., 100KB vs. 44GB) processing large data is challenging.

- not all jobs can be done in parallel (e.g., final sort to produce a single sorted file)

- easier to run mapreduce application on AWS EMR rather than running on local machine which requires lots of work to set things up. this allows us to focus on developing our application 
- I spent tremendous amount of time finding APIs that work with our hadoop version. Version compatibility is
  quite important.
 
*************************************************************************

5. Optimizations

*************************************************************************

- used static variables instead of local variables in map and reduce methods
- merged some map and reduce phases into one
- various local optimizations: for/while loop

*************************************************************************

6. Work 

*************************************************************************

contribution division
- Yifei: 
- Tae: implemented job 1,2,3,4 and tested/deployed the code with EMR on AWS, drew up the report, parsed XML file, computed the total number of pages, pagerank iteration 1 through 8, sorted the results, input/output file
management 

