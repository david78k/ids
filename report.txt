Description

1. The steps we took to develop the code:
- 1) non-MR on local machine: thought that it was going to be easy to convert
  non-MR to MR.
- 2) MR with Hadoop on local machine
- 3) EMR on AWS
- we also developed from small data to original huge data (ultimately the data given to
  us). For small data, we had to split the original data into small size
(first 100 lines, 1000 lines, 10000lines, 1million lines, and 5 million
lines). If we succeeded with small data, we incrementally grow the data size.
 
2. Difficulties we faced:
- parsing xml page was not easy. at first, we were trying to use a parser
library, but we couldn't get it right. After all, we parsed manually to get
what we want.

- too many specifications to follow

- data manipulation: sort, order, key-valye mapping, indexing,
  
- creating a jar file: combine separate files (external jars) into one was hard

- normalizing matrices to make the matrix column stochastic

- arranging (managing, adjusting) output directory layout is clumsy

- apache hadoop mapreduce related classes dependency complex, setup/import
  related classes

- mapreduce class type input/output, key/value class type so complex and
  confusing

- Although the required mapreduce API (version 1.0.3) that is quite old supports both old API (mapred.*) and new API (mapreduce.*), the old API doesn't work for some cases. should have
  been noted beforehand

- long procedure to run pagerank on EMR: installing many things to use command line tool for running client scripts

- testing and debugging are pains in the ass. every time the code has been changed, it has to
  be compiled/jared/uploaded. the worst thing is to wait for the cluster to be
initialized and run the jar application

- s3 API for Java to store output into s3bucket/results folder

- output file arrangement is messed up. naming files to meet the requirements
  like inlinks.out, n.out ...

- passing multiple arguments using emr command line tool was not easy

- Exception in thread "main"
  org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
file:/root/datascience/david78k-ids/results already exists
  It is so embarrassing that we can't overwrite existing directory. I had to
create a tmp directory for this.

- writable data structure for page that includes title and inlinks has complex
  format

- following the file formatting was the hardest part in this project, not the
  algorithms

- it was impossible to have two output classes during mapphase in pagerank
  job. Hence, I made use of the Page data structure.

- the data given is so large, I coudn't handle the disk space even on local
  cluster machines

3. How we solved the problems:
- to solve the output file conflict problems, I created files with the current
date and time.  
 
- As hadoop doesn' allow multiple input/output, i used string for both page
ranks and links to store/recover the link graph from map phase to reduce
phase. The problem with this approach is the precision of double number
(decimal point below 17 digits).
However, double number compuatation is not accurate anyway even with Double
class.

- shared variables for Mapper and Reducer with global variables in distributed
  mode while they work in single node

4. What we learned
- long process from input data to output data. there are too many steps, too
  many small things to take care of although the core algorithm is simple
- running the same code on a single local machine and distributed systemsa are somewhat different (e.g., different APIs for local file system and s3)
  static variable N is not set properly on EMR while it is set properly on
local environment
- processing the small data and large data is also different (e.g., 100KB vs.
  44GB) processing
  large data is challenging.
- not all jobs can be done in parallel (e.g., final sort to produce a single
  sorted file)
- easier to run mapreduce application on AWS EMR rather than running on local
machine which requires lots of work to set things up. this allows us to focus
on developing our application 
- I spent tremendous amount of time for finding APIs that work with our hadoop
  versoin. Version compatibility is
  quite important.
 
5. Optimizations
- static variables instead of local variables 
- merged some map and reduce phases into one
- sparse matrix (deprecated) for the adjacency matrix A as most of the entries are zero

6. Work contribution division
- Yifei: 
- Tae: implemented job 1,2,3,4 and tested/deployed the code with EMR on AWS
	parse XML file, compute the total number of pages, pagerank iteration 1 through 8, sort the results, input/output file management 


